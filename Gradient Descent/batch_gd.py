# -*- coding: utf-8 -*-
"""Batch_GD.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HWTuovkLQENA3eyyGUM8rYSYmQXelP7i

# Batch Gradient Descent
"""

import numpy as np

X = np.array([1, 2, 3, 4, 5], dtype=float)
y = np.array([3, 5, 7, 9, 11], dtype=float)

m = len(y)

def predict(x, theta0, theta1):
    return theta0 + theta1 * x

def compute_cost(X, y, theta0, theta1):
    predictions = predict(X, theta0, theta1)
    errors = predictions - y
    return np.mean(errors ** 2) / 2

def batch_gradient_descent(X, y, alpha=0.01, iterations=1000):
    theta0, theta1 = 0.0, 0.0
    m = len(y)

    for it in range(iterations):
        predictions = predict(X, theta0, theta1)
        errors = predictions - y

        grad0 = np.sum(errors) / m
        grad1 = np.sum(errors * X) / m

        theta0 -= alpha * grad0
        theta1 -= alpha * grad1

        if it % 100 == 0:
            cost = compute_cost(X, y, theta0, theta1)
            print(f"[Batch] Iter {it}: Cost={cost:.4f}, Œ∏0={theta0:.4f}, Œ∏1={theta1:.4f}")
    return theta0, theta1


print("\nüîÅ Running Batch Gradient Descent:")
batch_theta0, batch_theta1 = batch_gradient_descent(X, y)